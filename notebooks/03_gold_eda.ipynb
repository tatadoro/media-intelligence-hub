{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c7786d3",
   "metadata": {},
   "source": [
    "# 03. Gold layer EDA (v2): ключевые слова, персоны, гео и «кто что делает»\n",
    "\n",
    "Этот ноутбук анализирует **gold**-слой (Parquet) проекта *Media Intelligence Hub*:\n",
    "- качество данных и диапазон времени;\n",
    "- динамика ключевых слов;\n",
    "- топ персон и гео;\n",
    "- **действия персон**: какие глаголы чаще всего встречаются в предложениях, где упомянута персона.\n",
    "\n",
    "Примечание: извлечение «действий» сделано эвристически (без синтаксического парсинга), поэтому это хорошая\n",
    "базовая метрика для повестки, но не идеальный «кто-что-сделал». Ниже есть идеи, как усилить качество.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf94e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# optional: для извлечения глаголов\n",
    "try:\n",
    "    import pymorphy2\n",
    "    _MORPH_OK = True\n",
    "    morph = pymorphy2.MorphAnalyzer()\n",
    "except Exception as e:\n",
    "    _MORPH_OK = False\n",
    "    morph = None\n",
    "    print(\"[WARN] pymorphy2 недоступен, блок 'действия' будет пропущен:\", e)\n",
    "\n",
    "PROJECT_ROOT = Path.cwd()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d278e9ea",
   "metadata": {},
   "source": [
    "## 1) Загрузка данных\n",
    "\n",
    "По умолчанию берём **последние** gold-файлы:\n",
    "- RSS: `data/gold/articles_*_processed.parquet` (без `telegram`)\n",
    "- Telegram: `data/gold/articles_*_telegram_*_processed.parquet`\n",
    "\n",
    "Если хочешь анализировать конкретный файл — просто задай `RSS_GOLD` / `TG_GOLD` вручную.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5169a77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def latest_by_glob(pattern: str) -> Path:\n",
    "    matches = sorted(PROJECT_ROOT.glob(pattern), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    if not matches:\n",
    "        raise FileNotFoundError(f\"No files matched: {pattern}\")\n",
    "    return matches[0]\n",
    "\n",
    "# Авто-выбор последних файлов\n",
    "RSS_GOLD = None  # Path(\"data/gold/<your_file>.parquet\")\n",
    "TG_GOLD  = None  # Path(\"data/gold/<your_file>.parquet\")\n",
    "\n",
    "if RSS_GOLD is None:\n",
    "    # RSS: исключаем телеграмные файлы по имени\n",
    "    rss_candidates = [p for p in PROJECT_ROOT.glob(\"data/gold/articles_*_processed.parquet\") if \"telegram\" not in p.name]\n",
    "    if not rss_candidates:\n",
    "        print(\"[WARN] Не нашли RSS gold (data/gold/articles_*_processed.parquet без telegram).\")\n",
    "        RSS_GOLD = None\n",
    "    else:\n",
    "        RSS_GOLD = sorted(rss_candidates, key=lambda p: p.stat().st_mtime, reverse=True)[0]\n",
    "\n",
    "if TG_GOLD is None:\n",
    "    tg_candidates = list(PROJECT_ROOT.glob(\"data/gold/articles_*_telegram_*_processed.parquet\"))\n",
    "    if not tg_candidates:\n",
    "        print(\"[WARN] Не нашли Telegram gold (data/gold/articles_*_telegram_*_processed.parquet).\")\n",
    "        TG_GOLD = None\n",
    "    else:\n",
    "        TG_GOLD = sorted(tg_candidates, key=lambda p: p.stat().st_mtime, reverse=True)[0]\n",
    "\n",
    "RSS_GOLD, TG_GOLD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9eb418",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gold(path: Path, source_label: str) -> pd.DataFrame:\n",
    "    df = pd.read_parquet(path)\n",
    "    df = df.copy()\n",
    "    if \"source_group\" not in df.columns:\n",
    "        df[\"source_group\"] = source_label\n",
    "    return df\n",
    "\n",
    "frames = []\n",
    "if RSS_GOLD is not None:\n",
    "    frames.append(load_gold(RSS_GOLD, \"rss\"))\n",
    "if TG_GOLD is not None:\n",
    "    frames.append(load_gold(TG_GOLD, \"telegram\"))\n",
    "\n",
    "if not frames:\n",
    "    raise RuntimeError(\"Не удалось загрузить ни одного gold-файла. Проверь data/gold/ и пути.\")\n",
    "\n",
    "df = pd.concat(frames, ignore_index=True)\n",
    "print(\"rows:\", len(df))\n",
    "print(\"columns:\", len(df.columns))\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a85fab",
   "metadata": {},
   "source": [
    "## 2) Санити-чек: время, дубликаты id, пропуски\n",
    "\n",
    "Важно: дашборды «не обновляются» чаще всего потому, что **в ClickHouse пока нет новых публикаций**\n",
    "(например, Telegram/ RSS собирались за 25 декабря). Здесь проверяем диапазон времени прямо по gold.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26816b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# published_at может быть tz-naive (как в gold_to_clickhouse), приводим безопасно\n",
    "df[\"published_at\"] = pd.to_datetime(df[\"published_at\"], errors=\"coerce\")\n",
    "\n",
    "print(\"published_at min:\", df[\"published_at\"].min())\n",
    "print(\"published_at max:\", df[\"published_at\"].max())\n",
    "\n",
    "if \"id\" in df.columns:\n",
    "    s = df[\"id\"].astype(str).str.strip()\n",
    "    print(\"empty id:\", int((s==\"\").sum()))\n",
    "    print(\"unique id:\", s.nunique())\n",
    "    print(\"dupe ids:\", int(s.duplicated().sum()))\n",
    "\n",
    "# краткая сводка по источникам\n",
    "if \"source\" in df.columns:\n",
    "    display(df.groupby([\"source_group\", \"source\"]).size().sort_values(ascending=False).head(20))\n",
    "else:\n",
    "    display(df.groupby([\"source_group\"]).size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54470191",
   "metadata": {},
   "source": [
    "## 3) Ключевые слова: топ и динамика\n",
    "\n",
    "Предполагаем, что `keywords` в gold — строка с `;` (как в твоих витринах).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b5f4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explode_keywords(d: pd.DataFrame) -> pd.DataFrame:\n",
    "    if \"keywords\" not in d.columns:\n",
    "        return pd.DataFrame(columns=[\"published_at\", \"source_group\", \"keyword\"])\n",
    "    k = d[[\"published_at\", \"source_group\", \"keywords\"]].copy()\n",
    "    k[\"keywords\"] = k[\"keywords\"].fillna(\"\").astype(str)\n",
    "    k = k.assign(keyword=k[\"keywords\"].str.split(\";\")).explode(\"keyword\")\n",
    "    k[\"keyword\"] = k[\"keyword\"].fillna(\"\").astype(str).str.strip().str.lower()\n",
    "    k = k[k[\"keyword\"] != \"\"]\n",
    "    return k[[\"published_at\", \"source_group\", \"keyword\"]]\n",
    "\n",
    "kw = explode_keywords(df)\n",
    "kw.head(5), len(kw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c56a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_kw = kw[\"keyword\"].value_counts().head(30)\n",
    "top_kw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9930c352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Динамика по топ-10 ключевым словам\n",
    "TOP_N = 10\n",
    "top_set = set(top_kw.head(TOP_N).index.tolist())\n",
    "kw_top = kw[kw[\"keyword\"].isin(top_set)].copy()\n",
    "\n",
    "# округление до часа (можно заменить на 'min' / 'D')\n",
    "kw_top[\"ts\"] = kw_top[\"published_at\"].dt.floor(\"H\")\n",
    "\n",
    "ts = (\n",
    "    kw_top.groupby([\"ts\", \"keyword\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"cnt\")\n",
    "    .pivot(index=\"ts\", columns=\"keyword\", values=\"cnt\")\n",
    "    .fillna(0)\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "ax = ts.plot(kind=\"line\", figsize=(12, 5))\n",
    "ax.set_title(\"Keyword trends (top-10)\")\n",
    "ax.set_xlabel(\"time\")\n",
    "ax.set_ylabel(\"count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13689f38",
   "metadata": {},
   "source": [
    "## 4) Персоны и гео: топ\n",
    "\n",
    "Колонки ожидаются в формате `a;b;c` (как в gold).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452d4773",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explode_semicolon_col(d: pd.DataFrame, col: str, out_name: str) -> pd.DataFrame:\n",
    "    if col not in d.columns:\n",
    "        return pd.DataFrame(columns=[\"published_at\", \"source_group\", out_name])\n",
    "    x = d[[\"published_at\", \"source_group\", col]].copy()\n",
    "    x[col] = x[col].fillna(\"\").astype(str)\n",
    "    x = x.assign(**{out_name: x[col].str.split(\";\")}).explode(out_name)\n",
    "    x[out_name] = x[out_name].fillna(\"\").astype(str).str.strip().str.lower()\n",
    "    x = x[x[out_name] != \"\"]\n",
    "    return x[[\"published_at\", \"source_group\", out_name]]\n",
    "\n",
    "persons = explode_semicolon_col(df, \"persons\", \"person\")\n",
    "geo = explode_semicolon_col(df, \"geo\", \"geo_item\")\n",
    "\n",
    "persons[\"person\"].value_counts().head(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652ecff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo[\"geo_item\"].value_counts().head(30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5916cc3c",
   "metadata": {},
   "source": [
    "## 5) Действия персон (эвристика)\n",
    "\n",
    "Идея:\n",
    "1) берём текст (приоритет `nlp_text` → `clean_text` → `raw_text`);\n",
    "2) делим на предложения;\n",
    "3) если в предложении упомянута персона — собираем **глаголы** этого предложения;\n",
    "4) агрегируем по персоне.\n",
    "\n",
    "Это не полноценный синтаксический разбор, но уже даёт хороший «срез повестки»:\n",
    "- что чаще «делают» персоны в медиа;\n",
    "- какие действия доминируют для конкретной персоны.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c303335",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not _MORPH_OK:\n",
    "    raise RuntimeError(\"pymorphy2 не установлен, блок 'действия' выполнить нельзя.\")\n",
    "\n",
    "_TOKEN_RE = re.compile(r\"[A-Za-zА-Яа-яЁё-]+\")\n",
    "_SENT_SPLIT_RE = re.compile(r\"(?<=[\\.\\!\\?\\…])\\s+\")\n",
    "\n",
    "def _norm_text(s: str) -> str:\n",
    "    s = (s or \"\")\n",
    "    s = s.replace(\"ё\", \"е\")\n",
    "    return s\n",
    "\n",
    "def extract_verbs(sentence: str) -> list[str]:\n",
    "    sent = _norm_text(sentence).lower()\n",
    "    verbs: list[str] = []\n",
    "    for tok in _TOKEN_RE.findall(sent):\n",
    "        p = morph.parse(tok)[0]\n",
    "        if p.tag.POS in (\"VERB\", \"INFN\"):\n",
    "            verbs.append(p.normal_form)\n",
    "    return verbs\n",
    "\n",
    "def split_sentences(text: str) -> list[str]:\n",
    "    t = _norm_text(text).strip()\n",
    "    if not t:\n",
    "        return []\n",
    "    # сначала заменим переносы на пробелы, чтобы не ломать предложения\n",
    "    t = re.sub(r\"\\s+\", \" \", t)\n",
    "    return [s.strip() for s in _SENT_SPLIT_RE.split(t) if s.strip()]\n",
    "\n",
    "def iter_persons(persons_str: str) -> list[str]:\n",
    "    if not persons_str:\n",
    "        return []\n",
    "    out = [p.strip().lower() for p in persons_str.split(\";\")]\n",
    "    return [p for p in out if p]\n",
    "\n",
    "def choose_text_row(row: pd.Series) -> str:\n",
    "    for c in (\"nlp_text\", \"clean_text\", \"raw_text\"):\n",
    "        if c in row and pd.notna(row[c]) and str(row[c]).strip():\n",
    "            return str(row[c])\n",
    "    return \"\"\n",
    "\n",
    "def person_in_sentence(person: str, sentence: str) -> bool:\n",
    "    # базовая проверка: подстрока\n",
    "    sent = _norm_text(sentence).lower()\n",
    "    p = _norm_text(person).lower()\n",
    "    if p in sent:\n",
    "        return True\n",
    "    # fallback: для \"владимир зеленский\" проверим фамилию\n",
    "    parts = p.split()\n",
    "    if len(parts) >= 2:\n",
    "        return parts[-1] in sent\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0c92ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# собираем события \"person -> verbs\" построчно\n",
    "rows = []\n",
    "for _, r in df.iterrows():\n",
    "    persons_str = str(r.get(\"persons\") or \"\")\n",
    "    plist = iter_persons(persons_str)\n",
    "    if not plist:\n",
    "        continue\n",
    "\n",
    "    text = choose_text_row(r)\n",
    "    if not text:\n",
    "        continue\n",
    "\n",
    "    for sent in split_sentences(text):\n",
    "        verbs = extract_verbs(sent)\n",
    "        if not verbs:\n",
    "            continue\n",
    "\n",
    "        for p in plist:\n",
    "            if person_in_sentence(p, sent):\n",
    "                for v in verbs:\n",
    "                    rows.append((p, v))\n",
    "\n",
    "actions = pd.DataFrame(rows, columns=[\"person\", \"verb\"])\n",
    "actions.head(), len(actions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7fbac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Топ глаголов в повестке (только предложения с упоминанием персон)\n",
    "actions[\"verb\"].value_counts().head(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a343c652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Топ глаголов для топ-10 персон\n",
    "top_persons = persons[\"person\"].value_counts().head(10).index.tolist()\n",
    "for p in top_persons:\n",
    "    top_v = actions.loc[actions[\"person\"] == p, \"verb\"].value_counts().head(10)\n",
    "    print(\"\\n\", \"=\"*60)\n",
    "    print(\"PERSON:\", p)\n",
    "    print(top_v.to_string())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89865a82",
   "metadata": {},
   "source": [
    "## 6) Идеи, как улучшить качество «кто что делает»\n",
    "\n",
    "Если захочешь сделать метрику точнее (и «пристегнуть» её в пайплайн/ClickHouse), обычно идут по шагам:\n",
    "1) **Ограничить окно**: брать глаголы не из всего предложения, а в окне ±N токенов вокруг упоминания персоны.\n",
    "2) **Фильтр “шумных” глаголов**: сделать stop-list для «быть/стать/мочь/сказать/сообщить» и т.п.\n",
    "3) **Глаголы-события**: сгруппировать синонимы (например, “заявить/сообщить/сказать” → “заявить”).\n",
    "4) **Синтаксический разбор** (самый качественный вариант): определить, является ли персона субъектом глагола.\n",
    "   Для русского это обычно `spacy` (ru_core_news_lg) или `stanza`. Это тяжелее, но даёт “агент → действие”.\n",
    "\n",
    "Если ты скажешь, какой уровень качества нужен (эвристика / окно / синтаксис), я помогу оформить это как модуль\n",
    "`src/processing/actions.py` и добавить в gold как `actions_persons` (например, JSON: {\"персона\": [\"глагол1\", ...]}).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
